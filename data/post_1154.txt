作者hortune (enutroh)看板NTUcourse標題[評價] 107-1 李彥寰 最佳化演算法時間Sat Jan 19 02:02:47 2019
※ 本文是否可提供臺大同學轉作其他非營利用途？（須保留原作者 ID）
         （是／否／其他條件）：
        否

      哪一學年度修課：
        107-1
      ψ 授課教師 (若為多人合授請寫開課教師，以方便收錄)
        李彥寰
      λ 開課系所與授課對象 (是否為必修或通識課 / 內容是否與某些背景相關) 
        資工所
      δ 課程大概內容
        課程網頁: https://cool.ntu.edu.tw/courses/130
Ω 私心推薦指數(以五分計) ★★★★★
         ★ ★ ★ ★ ★  ★ ★ ★ ★ ★  ★ ★ ★ ★ ★
      η 上課用書(影印講義或是指定教科書)
        老師自己的投影片
      μ 上課方式(投影片、團體討論、老師教學風格)
        投影片在上前會放到NTU Cool
        上課就是投影片輔以白板
      σ 評分方式(給分甜嗎？是紮實分？)
        作業 40 % (取最高三次)
        期中考 20 % (期中考前的作業一定要寫，不寫會後悔QQ)
        Final Project 40 %
      ρ 考題型式、作業方式
        作業真的超Nice，基本上每次作業都是兩到三個大題，
        然後裡面的題目會一個接一個，逐步帶領你證出神秘的
        bound，這個真的是用心的老師才會這麼仔細的安排。

        不過老師作業一版通常都會有bug，所以要早點開始寫
        ，不然會被bug誤導得很慘，但老師回信修改的速度真
        的超快。

      ω 其它(是否注重出席率？如果為外系選修，需先有什麼基礎較好嗎？老師個性？
加簽習慣？嚴禁遲到等…)
        出席率 0
        建議修過線代，不然會有一點痛苦
        Hsuan-Tien ML, Shou-De ML, Hung-Yi ML之類的課，
        沒修過也沒差

        不過有修過 Chih-Jen OPTML(下學期停開QQ)的話，前面一兩份的投影
        片和第一次作業會滿舒服的，不過後面就沒差了。

      Ψ 總結
       在大大大ML時代，人人都是資料科學家，大家都是pytorch, tensorflow,
       keras, chainer,caffe疊積木大師，但你是否曾經想過，為何gradient
       descent會收斂，就算他會收斂，又會收斂的多快呢?那是否有比gradient
       descent還快的演算法呢?

       在堂課中，老師帶著大家從convex function簡單的性質開始，從定義
       開始，然後介紹了gradient descent然後又介紹了神一般的俄國人
       Nesterov的accelerated gradient descent，之後將gradient descent
       的式子寫成norm形式，開始想著是否能用其他的norm來做gradient descent?
       於是mirror descent就進入了眼簾，之後將convex function改成
       composite形式，出現了proximal gradient系列算法，然後你的期中
       考範圍就到這惹。

       期中考完後，就是Frank-Wolfe Method，之後稍微進入Online的世界
       介紹惹Online Mirror Descent。最後踏入Learning in games，從
       naive的follow-the-leader，加入神奇天眼的be-the-leader，最後
       進入follow-the-regularized leader.

       學期的最後，以Optimistic method劃下句點。

       好課不修嘛~ 而且期中考平均好像7x~


--
※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 140.112.215.99
※ 文章網址: https://www.ptt.cc/bbs/NTUcourse/M.1547834570.A.B57.html
推 liang1230: 你也太快...... 01/19 02:15
差點忘記寫
※ 編輯: hortune (140.112.215.99), 01/19/2019 02:15:57
→ liang1230: 樓主神人 小弟常常讀到懷疑人生 01/19 02:19
推 liang1230: 補個：王奕翔教授也會來旁聽這門課 01/19 02:21
→ liang1230: 下學期的MPML如果跟去年一樣 在gradient descent的部 01/19 02:21
→ liang1230: 分跟期中考前重複蠻多的 只是上的超快 01/19 02:21
→ liang1230: 但覺得若對ML為什麼會work的原理有興趣也是推薦大家修 01/19 02:23
→ liang1230: 個 但是作業可能更懷疑人生啦 從網路上抄可能都有抄個 01/19 02:23
→ liang1230: 三天三夜才抄的完 01/19 02:23
推 yiefaung: 推 01/19 04:02
推 xavierqqqq: 請問除了線代就夠了嗎 沒資料結構跟演算法的基礎會聽 01/19 12:16
→ xavierqqqq: 不懂嗎 01/19 12:16
推 exe1023: 回樓上演算法只要懂bigO那些的意思就好 資料結構不用 聽 01/19 12:48
→ exe1023: 不懂通常只是自己數學檔次不夠 01/19 12:48
推 xavierqqqq: 3Q 01/19 14:38
推 empennage98: 除了線代之外，還要會操作多變量微積分 01/19 15:23
推 CharlieL: 推寰神 01/19 21:03
推 MataAshita: 請問被當+停修的人多嗎? 01/19 21:20
是滿多的
不過大家停修的原因都不太一樣
有人是拿到廣告公司offer就休學了
有人是拿到國外visiting research offer就停修了
有人是覺得自己的研究領域只需要前面一半就停修惹
不過期中考人數是一開始人數的一半吧XD
旁聽眾多的一門課
→ MataAshita: 田神出沒!! 01/19 21:20
推 rrro: 推寰神！ 01/19 21:44
推 CKNTUErnie: 哇 神串留名 01/19 21:50
推 unmolk: 樓上上rrro 01/20 03:31
推 dQoQb: 推寰神！ 01/20 13:47
※ 編輯: hortune (140.112.16.133), 01/20/2019 17:46:54
推 PyTorch: 砍一半 那跟王神的機器學習數學原理有得拼 01/20 18:35
